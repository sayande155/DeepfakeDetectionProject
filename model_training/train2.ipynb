{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f872890a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello \n"
     ]
    }
   ],
   "source": [
    "print(\"Hello \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c9b2594",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69dcb7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install efficientnet_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7689e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Running on\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b89d886",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 2050'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cbb889b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.1+cu118\n",
      "11.8\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0e12180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total real videos for training: 585\n",
      "Total fake videos for training: 5634\n"
     ]
    }
   ],
   "source": [
    "# DATASET PATH\n",
    "\n",
    "# Real and Fake Video list json path \n",
    " \n",
    "fake_path = \"D:/Desktop/FinalYearProject/DataSets/celeb_df_face_cropped/valid_fake_videos.json\"\n",
    "real_path = \"D:/Desktop/FinalYearProject/DataSets/celeb_df_face_cropped/valid_real_videos.json\"\n",
    "\n",
    "# Load lists from JSON\n",
    "valid_real_videos = []\n",
    "valid_fake_videos = []\n",
    "\n",
    "with open(real_path, 'r') as f:\n",
    "    valid_real_videos = json.load(f)\n",
    "\n",
    "with open(fake_path, 'r') as f:\n",
    "    valid_fake_videos = json.load(f)\n",
    "\n",
    "# Paths to real and fake video folders on Google Drive\n",
    "celeb_df_real_path = 'D:/Desktop/FinalYearProject/DataSets/celeb_df_face_cropped/real_face_only224/real_face_only224'\n",
    "celeb_df_fake_path = 'D:/Desktop/FinalYearProject/DataSets/celeb_df_face_cropped/fake_face_only224/fake_face_only224'\n",
    "\n",
    "# Reconstruct full paths\n",
    "valid_real_videos_path = [os.path.normpath(os.path.join(celeb_df_real_path, name)) for name in valid_real_videos]\n",
    "valid_fake_videos_path = [os.path.normpath(os.path.join(celeb_df_fake_path, name)) for name in valid_fake_videos]\n",
    "\n",
    "print(f\"Total real videos for training: {len(valid_real_videos_path)}\")\n",
    "print(f\"Total fake videos for training: {len(valid_fake_videos_path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2070941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Desktop\\FinalYearProject\\DataSets\\celeb_df_face_cropped\\real_face_only224\\real_face_only224\\id0_0000.mp4\n",
      "D:\\Desktop\\FinalYearProject\\DataSets\\celeb_df_face_cropped\\fake_face_only224\\fake_face_only224\\id0_id16_0000.mp4\n"
     ]
    }
   ],
   "source": [
    "print(valid_real_videos_path[0])\n",
    "print(valid_fake_videos_path[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8638e24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d87b8fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepFakeDataset(Dataset):\n",
    "    def __init__(self, video_paths, labels, transform=None, num_frames=16):\n",
    "        self.video_paths = video_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.num_frames = num_frames\n",
    "\n",
    "    def read_frames(self, video_path):\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        frame_indices = np.linspace(0, total_frames - 1, self.num_frames).astype(int)   #\n",
    "        frames = []\n",
    "\n",
    "        for idx in frame_indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                continue\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            if self.transform:\n",
    "                frame = self.transform(image=frame)['image']\n",
    "            frames.append(frame)\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "        if len(frames) < self.num_frames:\n",
    "            # pad missing frames with black images\n",
    "            for _ in range(self.num_frames - len(frames)):\n",
    "                frames.append(torch.zeros_like(frames[0]))\n",
    "\n",
    "        return torch.stack(frames)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            video_tensor = self.read_frames(self.video_paths[idx])\n",
    "            label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "            return video_tensor, label\n",
    "        except Exception as e:\n",
    "            print(f\"Failed loading video: {self.video_paths[idx]}, Error: {e}\")\n",
    "            return self.__getitem__((idx + 1) % len(self))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "425edd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA TRANSFORMS\n",
    "transform = A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "788f8012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine video paths and labels\n",
    "video_paths = valid_real_videos_path + valid_fake_videos_path\n",
    "labels = [0]*len(valid_real_videos_path) + [1]*len(valid_fake_videos_path)\n",
    "\n",
    "# Use train_test_split to shuffle and split\n",
    "train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
    "    video_paths, labels, test_size=0.2, stratify=labels, random_state=10\n",
    ")\n",
    "\n",
    "# Create datasets and loaders\n",
    "train_dataset = DeepFakeDataset(train_paths, train_labels, transform)\n",
    "val_dataset = DeepFakeDataset(val_paths, val_labels, transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c20c39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL COMPONENTS\n",
    "class TemporalAttention(nn.Module):\n",
    "    def __init__(self, feature_dim):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Linear(feature_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, time, features)\n",
    "        weights = F.softmax(self.attention(x), dim=1)\n",
    "        return torch.sum(weights * x, dim=1)\n",
    "\n",
    "\n",
    "class DeepFakeDetector(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "        self.feature_extractor._fc = nn.Identity()\n",
    "        self.lstm = nn.LSTM(input_size=1280, hidden_size=256, num_layers=1, batch_first=True, bidirectional=True)\n",
    "        self.attention = TemporalAttention(512)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C, H, W = x.shape\n",
    "        x = x.view(B*T, C, H, W)\n",
    "        with torch.no_grad():  # freeze feature extractor to reduce memory\n",
    "            feats = self.feature_extractor(x)\n",
    "        feats = feats.view(B, T, -1)\n",
    "        lstm_out, _ = self.lstm(feats)\n",
    "        attn_out = self.attention(lstm_out)\n",
    "        return self.classifier(attn_out).squeeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f33504c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "\n",
    "    loop = tqdm(dataloader, desc=\"Training\", leave=True)\n",
    "    for inputs, labels in loop:\n",
    "        inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        correct += (preds == labels).sum().item()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    epoch_acc = correct / len(dataloader.dataset)\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "\n",
    "    loop = tqdm(dataloader, desc=\"Validation\", leave=True)\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loop:\n",
    "            inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            correct += (preds == labels).sum().item()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    epoch_acc = correct / len(dataloader.dataset)\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3fdb11e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    }
   ],
   "source": [
    "# START TRAINING\n",
    "model = DeepFakeDetector().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "154f75d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ebc08ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare save path\n",
    "model_dir = 'D:/Desktop/FinalYearProject/models/rPPG_model'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "save_path = os.path.join(model_dir, 'best_model_celeb_df.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ea02a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 622/622 [08:17<00:00,  1.25it/s]\n",
      "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 156/156 [02:04<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3050, Accuracy: 0.9053\n",
      "Val   Loss: 0.2260, Accuracy: 0.9164\n",
      "ðŸ’¾ Model saved to Google Drive with Val Accuracy: 0.9164\n",
      "\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 622/622 [07:43<00:00,  1.34it/s]\n",
      "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 156/156 [01:49<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2285, Accuracy: 0.9192\n",
      "Val   Loss: 0.1950, Accuracy: 0.9389\n",
      "ðŸ’¾ Model saved to Google Drive with Val Accuracy: 0.9389\n",
      "\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 622/622 [07:31<00:00,  1.38it/s]\n",
      "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 156/156 [01:47<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2006, Accuracy: 0.9303\n",
      "Val   Loss: 0.1935, Accuracy: 0.9357\n",
      "\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 622/622 [07:27<00:00,  1.39it/s]\n",
      "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 156/156 [01:45<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1883, Accuracy: 0.9323\n",
      "Val   Loss: 0.2322, Accuracy: 0.9252\n",
      "\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 622/622 [07:49<00:00,  1.32it/s]\n",
      "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 156/156 [01:50<00:00,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1685, Accuracy: 0.9383\n",
      "Val   Loss: 0.1856, Accuracy: 0.9293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "EPOCHS = 5\n",
    "best_val_acc = 0.0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
    "\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"Val   Loss: {val_loss:.4f}, Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_accuracy': best_val_acc\n",
    "        }, save_path)\n",
    "        print(f\"ðŸ’¾ Model saved to Google Drive with Val Accuracy: {best_val_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
