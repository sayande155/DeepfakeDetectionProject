{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9ceb48e",
   "metadata": {},
   "source": [
    "# Model Training with Heart Rate extracted from rPPG along with other spatial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c53d9a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e190fb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea7a4a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Running on\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "074a9b01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 2050'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6083b653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.1+cu118\n",
      "11.8\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9798406a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real and Fake Video list json path \n",
    " \n",
    "fake_path = \"D:/Desktop/FinalYearProject/DataSets/celeb_df_face_cropped/valid_fake_videos.json\"\n",
    "real_path = \"D:/Desktop/FinalYearProject/DataSets/celeb_df_face_cropped/valid_real_videos.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd15bc26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total real videos for training: 585\n",
      "Total fake videos for training: 5634\n"
     ]
    }
   ],
   "source": [
    "# Load lists from JSON\n",
    "valid_real_videos = []\n",
    "valid_fake_videos = []\n",
    "\n",
    "with open(real_path, 'r') as f:\n",
    "    valid_real_videos = json.load(f)\n",
    "\n",
    "with open(fake_path, 'r') as f:\n",
    "    valid_fake_videos = json.load(f)\n",
    "\n",
    "# Paths to real and fake video folders on Google Drive\n",
    "celeb_df_real_path = 'D:/Desktop/FinalYearProject/DataSets/celeb_df_face_cropped/real_face_only224/real_face_only224'\n",
    "celeb_df_fake_path = 'D:/Desktop/FinalYearProject/DataSets/celeb_df_face_cropped/fake_face_only224/fake_face_only224'\n",
    "\n",
    "# Reconstruct full paths\n",
    "valid_real_videos_path = [os.path.normpath(os.path.join(celeb_df_real_path, name)) for name in valid_real_videos]\n",
    "valid_fake_videos_path = [os.path.normpath(os.path.join(celeb_df_fake_path, name)) for name in valid_fake_videos]\n",
    "\n",
    "print(f\"Total real videos for training: {len(valid_real_videos_path)}\")\n",
    "print(f\"Total fake videos for training: {len(valid_fake_videos_path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c520983e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Desktop\\FinalYearProject\\DataSets\\celeb_df_face_cropped\\real_face_only224\\real_face_only224\\id0_0000.mp4\n",
      "D:\\Desktop\\FinalYearProject\\DataSets\\celeb_df_face_cropped\\fake_face_only224\\fake_face_only224\\id0_id16_0000.mp4\n"
     ]
    }
   ],
   "source": [
    "print(valid_real_videos_path[0])\n",
    "print(valid_fake_videos_path[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94b7ab72",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b382bba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hr values path\n",
    "\n",
    "real_hr_csv = \"D:/Desktop/FinalYearProject/DataSets/celeb_df_hr/real_heart_rate.csv\"\n",
    "fake_hr_csv = \"D:/Desktop/FinalYearProject/DataSets/celeb_df_hr/fake_heart_rate.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0c1695e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hr mapping function from csv\n",
    "\n",
    "def load_hr_from_csv(real_hr_csv, fake_hr_csv):\n",
    "    hr_map = {}\n",
    "    for path in ([real_hr_csv, fake_hr_csv]):\n",
    "        df = pd.read_csv(path)\n",
    "        df.columns = [c.strip().lower() for c in df.columns]\n",
    "        for _, row in df.iterrows():\n",
    "            video = str(row[\"video\"]).strip()\n",
    "            hr_map[video] = {\n",
    "                'hr_time' : float(row.get(\"hr_time\", 0)),\n",
    "                'hr_freq' : float(row.get(\"hr_freq\", 0))\n",
    "            }\n",
    "    \n",
    "    return hr_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef8e731c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6219"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hr_map = load_hr_from_csv(real_hr_csv, fake_hr_csv)\n",
    "len(hr_map)\n",
    "# hr_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69101ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset With HR features\n",
    "\n",
    "\n",
    "class DeepfakeDataset(Dataset):\n",
    "\n",
    "    def __init__(self, video_paths, labels, hr_map, transform=None, num_frames=16):\n",
    "        self.video_paths = video_paths\n",
    "        self.labels = labels\n",
    "        self.hr_map = hr_map\n",
    "        self.transform = transform\n",
    "        self.num_frames = num_frames\n",
    "\n",
    "    def read_frame(self, video_path):\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "        # if unreadable, return zeros\n",
    "        if total_frames is None or total_frames <= 0:\n",
    "            cap.release()\n",
    "            return torch.zeros((self.num_frames, 3, 224, 224), dtype=torch.float32)\n",
    "\n",
    "        frame_indices = np.linspace(0, total_frames - 1, self.num_frames).astype(int)\n",
    "        frames = []\n",
    "\n",
    "        for index in frame_indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, index)\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                continue\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            if self.transform:\n",
    "                frame = self.transform(image=frame)[\"image\"]\n",
    "            frames.append(frame)\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "        if len(frames) < self.num_frames:\n",
    "            pad = self.num_frames - len(frames)\n",
    "            filler = torch.zeros(3, 224, 224, dtype=torch.float32)\n",
    "            frames.extend([filler] * pad)\n",
    "\n",
    "        return torch.stack(frames)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        try:\n",
    "            video_path = self.video_paths[index]\n",
    "            video_tensor = self.read_frame(video_path)\n",
    "\n",
    "            video_name = os.path.basename(video_path)\n",
    "            hr = self.hr_map.get(video_name, {\"hr_time\": 0.0, \"hr_freq\": 0.0})\n",
    "\n",
    "            hr_time  = float(hr.get(\"hr_time\", 0.0))\n",
    "            hr_freq  = float(hr.get(\"hr_freq\", 0.0))\n",
    "\n",
    "            # replace NaN/inf with 60\n",
    "            if not np.isfinite(hr_time): hr_time = 60.0\n",
    "            if not np.isfinite(hr_freq): hr_freq = 60.0\n",
    "\n",
    "            # simple scaling to ~[0,1]; tweak if you know your ranges\n",
    "            hr_time /= 200.0\n",
    "            hr_freq /= 200.0\n",
    "\n",
    "            hr_tensor = torch.tensor([hr_time, hr_freq], dtype=torch.float32)\n",
    "\n",
    "            label = torch.tensor(self.labels[index], dtype=torch.float32)\n",
    "\n",
    "            return video_tensor, hr_tensor, label\n",
    "        except Exception as e:\n",
    "            print(f\"Failed loading video: {self.video_paths[index]}, Error: {e}\")\n",
    "            return self.__getitem__((index + 1) % len(self))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6beb704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms\n",
    "\n",
    "# transform = A.Compose([\n",
    "#     A.Resize(224, 224),\n",
    "#     A.Normalize(),\n",
    "#     ToTensorV2(),\n",
    "# ])\n",
    "\n",
    "transform = A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e7b84a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Loading\n",
    "\n",
    "# Combine video paths and labels\n",
    "video_paths = valid_real_videos_path + valid_fake_videos_path\n",
    "labels = [0] * len(valid_real_videos_path) + [1] * len(valid_fake_videos_path)\n",
    "\n",
    "# train_test_split\n",
    "train_path, test_path, train_labels, test_labels = train_test_split(\n",
    "    video_paths, labels, test_size=0.2, stratify=labels, random_state=10\n",
    ")\n",
    "\n",
    "# Create dataset and dataloaders\n",
    "train_dataset = DeepfakeDataset(\n",
    "    train_path, train_labels, hr_map=hr_map, transform=transform, num_frames=16\n",
    ")\n",
    "test_dataset = DeepfakeDataset(\n",
    "    test_path, test_labels, hr_map=hr_map, transform=transform, num_frames=16\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=8, shuffle=True, num_workers=0, pin_memory=True\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=8, shuffle=False, num_workers=0, pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c760e3bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "622"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e0704ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset.__getitem__(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2a9a909c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deepfake Model with HR \n",
    "\n",
    "class TemporalAttention(nn.Module):\n",
    "    def __init__(self, feature_dim):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Linear(feature_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, time, features)\n",
    "        weights = F.softmax(self.attention(x), dim=1)\n",
    "        return torch.sum(weights * x, dim=1)\n",
    "    \n",
    "\n",
    "class DeepfakeDetectorHR(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "        self.feature_extractor._fc = nn.Identity()\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=1280, hidden_size=256, num_layers=1, batch_first=True, bidirectional=True)\n",
    "        self.attention = TemporalAttention(512) \n",
    "\n",
    "        # HR branch\n",
    "        self.hr_fc = nn.Sequential(\n",
    "            nn.Linear(2,32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )   \n",
    "\n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 + 32,  128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128,1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, hr):\n",
    "        B, T, C, H, W = x.shape\n",
    "        x = x.view(B*T, C, H, W)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            feats = self.feature_extractor(x)\n",
    "        feats = feats.view(B, T, -1)\n",
    "\n",
    "        lstm_out, _ = self.lstm(feats)\n",
    "        attn_out = self.attention(lstm_out) # (B, 512)\n",
    "\n",
    "        hr_feat = self.hr_fc(hr)    # (B, 32)\n",
    "        fused = torch.cat([attn_out, hr_feat], dim=1)\n",
    "\n",
    "        return self.classifier(fused).squeeze(1)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "40268a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and Validate\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    first_batch = True\n",
    "    \n",
    "    loop = tqdm(dataloader, desc=\"Training \", leave=True)\n",
    "    for inputs, hr, labels in loop:\n",
    "        inputs, hr, labels = inputs.to(device), hr.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        outputs = model(inputs, hr)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        correct += (preds == labels).sum().item()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        if first_batch:\n",
    "            with torch.no_grad():\n",
    "                print(f\"[sanity] batch loss={loss.item():.4f}, \"\n",
    "                      f\"inputs={inputs.dtype}/{inputs.min().item():.3f}..{inputs.max().item():.3f}, \"\n",
    "                      f\"hr={hr[0].tolist()}\")\n",
    "            first_batch = False\n",
    "\n",
    "    return running_loss / len(dataloader.dataset),  correct / len(dataloader.dataset)\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loop = tqdm(dataloader, desc=\"Validation\", leave=True)\n",
    "        for inputs, hr, labels in loop:\n",
    "            inputs, hr, labels = inputs.to(device), hr.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs, hr)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            correct += (preds == labels).sum().item()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        return running_loss / len(dataloader.dataset), correct / len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "797e6c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    }
   ],
   "source": [
    "# START TRAINING\n",
    "model = DeepfakeDetectorHR().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0fb5f1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e8fe3e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare save path\n",
    "model_dir = 'D:/Desktop/FinalYearProject/models/rPPG_model'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "save_path = os.path.join(model_dir, 'best_model_celeb_df.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8553e37a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training :   0%|          | 1/622 [00:01<14:38,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sanity] batch loss=0.6881, inputs=torch.float32/-2.118..2.249, hr=[0.6207000017166138, 0.47679999470710754]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training : 100%|██████████| 622/622 [09:13<00:00,  1.12it/s]\n",
      "Validation: 100%|██████████| 156/156 [02:11<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 0.2975 | Train Acc 0.9079 | Val Loss 0.2295 | Val Acc 0.9228\n",
      "💾 Best model saved to D:/Desktop/FinalYearProject/models/rPPG_model\\best_model_celeb_df.pth with Val Acc: 0.9228\n",
      "\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training :   0%|          | 1/622 [00:00<09:48,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sanity] batch loss=0.0851, inputs=torch.float32/-2.118..2.518, hr=[0.6000000238418579, 0.35760000348091125]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training : 100%|██████████| 622/622 [08:57<00:00,  1.16it/s]\n",
      "Validation: 100%|██████████| 156/156 [02:09<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 0.2348 | Train Acc 0.9242 | Val Loss 0.2210 | Val Acc 0.9349\n",
      "💾 Best model saved to D:/Desktop/FinalYearProject/models/rPPG_model\\best_model_celeb_df.pth with Val Acc: 0.9349\n",
      "\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training :   0%|          | 1/622 [00:00<08:11,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sanity] batch loss=0.0615, inputs=torch.float32/-2.118..2.431, hr=[0.692300021648407, 0.5664499998092651]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training : 100%|██████████| 622/622 [08:54<00:00,  1.16it/s]\n",
      "Validation: 100%|██████████| 156/156 [02:10<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 0.2120 | Train Acc 0.9290 | Val Loss 0.2117 | Val Acc 0.9317\n",
      "\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training :   0%|          | 1/622 [00:00<09:04,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sanity] batch loss=0.1213, inputs=torch.float32/-2.118..2.198, hr=[0.6428499817848206, 0.2980000078678131]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training : 100%|██████████| 622/622 [08:06<00:00,  1.28it/s]\n",
      "Validation: 100%|██████████| 156/156 [01:47<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 0.1973 | Train Acc 0.9325 | Val Loss 0.1806 | Val Acc 0.9349\n",
      "\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training :   0%|          | 1/622 [00:00<09:09,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sanity] batch loss=0.0333, inputs=torch.float32/-2.118..2.379, hr=[0.75, 0.2980000078678131]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training : 100%|██████████| 622/622 [07:40<00:00,  1.35it/s]\n",
      "Validation: 100%|██████████| 156/156 [01:46<00:00,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 0.1920 | Train Acc 0.9357 | Val Loss 0.2092 | Val Acc 0.9357\n",
      "💾 Best model saved to D:/Desktop/FinalYearProject/models/rPPG_model\\best_model_celeb_df.pth with Val Acc: 0.9357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "EPOCHS = 5\n",
    "best_val_acc = 0.0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
    "\n",
    "    train_loss, train_acc = train_one_epoch(model, train_dataloader, optimizer, criterion, device)\n",
    "    val_loss, val_acc     = validate(model, test_dataloader, criterion, device)\n",
    "\n",
    "    print(f\"Train Loss {train_loss:.4f} | Train Acc {train_acc:.4f} | \"\n",
    "          f\"Val Loss {val_loss:.4f} | Val Acc {val_acc:.4f}\")\n",
    "\n",
    "    # Save best model checkpoint\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_accuracy': best_val_acc\n",
    "        }, save_path)\n",
    "        print(f\"💾 Best model saved to {save_path} with Val Acc: {best_val_acc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
