{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9ceb48e",
   "metadata": {},
   "source": [
    "# Model Training with Heart Rate extracted from rPPG along with other spatial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c53d9a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e190fb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea7a4a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Running on\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "074a9b01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 2050'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6083b653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.1+cu118\n",
      "11.8\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9798406a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real and Fake Video list json path \n",
    " \n",
    "fake_path = \"D:/Desktop/FinalYearProject/DataSets/celeb_df_face_cropped/valid_fake_videos.json\"\n",
    "real_path = \"D:/Desktop/FinalYearProject/DataSets/celeb_df_face_cropped/valid_real_videos.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd15bc26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total real videos for training: 585\n",
      "Total fake videos for training: 5634\n"
     ]
    }
   ],
   "source": [
    "# Load lists from JSON\n",
    "valid_real_videos = []\n",
    "valid_fake_videos = []\n",
    "\n",
    "with open(real_path, 'r') as f:\n",
    "    valid_real_videos = json.load(f)\n",
    "\n",
    "with open(fake_path, 'r') as f:\n",
    "    valid_fake_videos = json.load(f)\n",
    "\n",
    "# Paths to real and fake video folders on Google Drive\n",
    "celeb_df_real_path = 'D:/Desktop/FinalYearProject/DataSets/celeb_df_face_cropped/real_face_only224/real_face_only224'\n",
    "celeb_df_fake_path = 'D:/Desktop/FinalYearProject/DataSets/celeb_df_face_cropped/fake_face_only224/fake_face_only224'\n",
    "\n",
    "# Reconstruct full paths\n",
    "valid_real_videos_path = [os.path.normpath(os.path.join(celeb_df_real_path, name)) for name in valid_real_videos]\n",
    "valid_fake_videos_path = [os.path.normpath(os.path.join(celeb_df_fake_path, name)) for name in valid_fake_videos]\n",
    "\n",
    "print(f\"Total real videos for training: {len(valid_real_videos_path)}\")\n",
    "print(f\"Total fake videos for training: {len(valid_fake_videos_path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c520983e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Desktop\\FinalYearProject\\DataSets\\celeb_df_face_cropped\\real_face_only224\\real_face_only224\\id0_0000.mp4\n",
      "D:\\Desktop\\FinalYearProject\\DataSets\\celeb_df_face_cropped\\fake_face_only224\\fake_face_only224\\id0_id16_0000.mp4\n"
     ]
    }
   ],
   "source": [
    "print(valid_real_videos_path[0])\n",
    "print(valid_fake_videos_path[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94b7ab72",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b382bba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hr values path\n",
    "\n",
    "real_hr_csv = \"D:/Desktop/FinalYearProject/DataSets/celeb_df_hr/real_heart_rate.csv\"\n",
    "fake_hr_csv = \"D:/Desktop/FinalYearProject/DataSets/celeb_df_hr/fake_heart_rate.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0c1695e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hr mapping function from csv\n",
    "\n",
    "def load_hr_from_csv(real_hr_csv, fake_hr_csv):\n",
    "    hr_map = {}\n",
    "    for path in ([real_hr_csv, fake_hr_csv]):\n",
    "        df = pd.read_csv(path)\n",
    "        df.columns = [c.strip().lower() for c in df.columns]\n",
    "        for _, row in df.iterrows():\n",
    "            video = str(row[\"video\"]).strip()\n",
    "            hr_map[video] = {\n",
    "                'hr_time' : float(row.get(\"hr_time\", 0)),\n",
    "                'hr_freq' : float(row.get(\"hr_freq\", 0))\n",
    "            }\n",
    "    \n",
    "    return hr_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef8e731c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6219"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hr_map = load_hr_from_csv(real_hr_csv, fake_hr_csv)\n",
    "len(hr_map)\n",
    "# hr_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69101ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset With HR features\n",
    "\n",
    "\n",
    "class DeepfakeDataset(Dataset):\n",
    "\n",
    "    def __init__(self, video_paths, labels, hr_map, transform=None, num_frames=16):\n",
    "        self.video_paths = video_paths\n",
    "        self.labels = labels\n",
    "        self.hr_map = hr_map\n",
    "        self.transform = transform\n",
    "        self.num_frames = num_frames\n",
    "\n",
    "    def read_frame(self, video_path):\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "        # if unreadable, return zeros\n",
    "        if total_frames is None or total_frames <= 0:\n",
    "            cap.release()\n",
    "            return torch.zeros((self.num_frames, 3, 224, 224), dtype=torch.float32)\n",
    "\n",
    "        frame_indices = np.linspace(0, total_frames - 1, self.num_frames).astype(int)\n",
    "        frames = []\n",
    "\n",
    "        for index in frame_indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, index)\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                continue\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            if self.transform:\n",
    "                frame = self.transform(image=frame)[\"image\"]\n",
    "            frames.append(frame)\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "        if len(frames) < self.num_frames:\n",
    "            pad = self.num_frames - len(frames)\n",
    "            filler = torch.zeros(3, 224, 224, dtype=torch.float32)\n",
    "            frames.extend([filler] * pad)\n",
    "\n",
    "        return torch.stack(frames)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        try:\n",
    "            video_path = self.video_paths[index]\n",
    "            video_tensor = self.read_frame(video_path)\n",
    "\n",
    "            video_name = os.path.basename(video_path)\n",
    "            hr = self.hr_map.get(video_name, {\"hr_time\": 0.0, \"hr_freq\": 0.0})\n",
    "\n",
    "            hr_time  = float(hr.get(\"hr_time\", 0.0))\n",
    "            hr_freq  = float(hr.get(\"hr_freq\", 0.0))\n",
    "\n",
    "            # replace NaN/inf with 60\n",
    "            if not np.isfinite(hr_time): hr_time = 60.0\n",
    "            if not np.isfinite(hr_freq): hr_freq = 60.0\n",
    "\n",
    "            # simple scaling to ~[0,1]; tweak if you know your ranges\n",
    "            hr_time /= 200.0\n",
    "            hr_freq /= 200.0\n",
    "\n",
    "            hr_tensor = torch.tensor([hr_time, hr_freq], dtype=torch.float32)\n",
    "\n",
    "            label = torch.tensor(self.labels[index], dtype=torch.float32)\n",
    "\n",
    "            return video_tensor, hr_tensor, label\n",
    "        except Exception as e:\n",
    "            print(f\"Failed loading video: {self.video_paths[index]}, Error: {e}\")\n",
    "            return self.__getitem__((index + 1) % len(self))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6beb704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms\n",
    "\n",
    "# transform = A.Compose([\n",
    "#     A.Resize(224, 224),\n",
    "#     A.Normalize(),\n",
    "#     ToTensorV2(),\n",
    "# ])\n",
    "\n",
    "transform = A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e7b84a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Loading\n",
    "\n",
    "# Combine video paths and labels\n",
    "video_paths = valid_real_videos_path + valid_fake_videos_path\n",
    "labels = [0] * len(valid_real_videos_path) + [1] * len(valid_fake_videos_path)\n",
    "\n",
    "# train_test_split\n",
    "train_path, test_path, train_labels, test_labels = train_test_split(\n",
    "    video_paths, labels, test_size=0.2, stratify=labels, random_state=10\n",
    ")\n",
    "\n",
    "# Create dataset and dataloaders\n",
    "train_dataset = DeepfakeDataset(\n",
    "    train_path, train_labels, hr_map=hr_map, transform=transform, num_frames=16\n",
    ")\n",
    "test_dataset = DeepfakeDataset(\n",
    "    test_path, test_labels, hr_map=hr_map, transform=transform, num_frames=16\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=8, shuffle=True, num_workers=0, pin_memory=True\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=8, shuffle=False, num_workers=0, pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c760e3bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "622"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e0704ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset.__getitem__(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2a9a909c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deepfake Model with HR \n",
    "\n",
    "class TemporalAttention(nn.Module):\n",
    "    def __init__(self, feature_dim):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Linear(feature_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, time, features)\n",
    "        weights = F.softmax(self.attention(x), dim=1)\n",
    "        return torch.sum(weights * x, dim=1)\n",
    "    \n",
    "\n",
    "class DeepfakeDetectorHR(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "        self.feature_extractor._fc = nn.Identity()\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=1280, hidden_size=256, num_layers=1, batch_first=True, bidirectional=True)\n",
    "        self.attention = TemporalAttention(512) \n",
    "\n",
    "        # HR branch\n",
    "        self.hr_fc = nn.Sequential(\n",
    "            nn.Linear(2,32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )   \n",
    "\n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 + 32,  128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128,1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, hr):\n",
    "        B, T, C, H, W = x.shape\n",
    "        x = x.view(B*T, C, H, W)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            feats = self.feature_extractor(x)\n",
    "        feats = feats.view(B, T, -1)\n",
    "\n",
    "        lstm_out, _ = self.lstm(feats)\n",
    "        attn_out = self.attention(lstm_out) # (B, 512)\n",
    "\n",
    "        hr_feat = self.hr_fc(hr)    # (B, 32)\n",
    "        fused = torch.cat([attn_out, hr_feat], dim=1)\n",
    "\n",
    "        return self.classifier(fused).squeeze(1)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "40268a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and Validate\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    first_batch = True\n",
    "    \n",
    "    loop = tqdm(dataloader, desc=\"Training \", leave=True)\n",
    "    for inputs, hr, labels in loop:\n",
    "        inputs, hr, labels = inputs.to(device), hr.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        outputs = model(inputs, hr)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        correct += (preds == labels).sum().item()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        if first_batch:\n",
    "            with torch.no_grad():\n",
    "                print(f\"[sanity] batch loss={loss.item():.4f}, \"\n",
    "                      f\"inputs={inputs.dtype}/{inputs.min().item():.3f}..{inputs.max().item():.3f}, \"\n",
    "                      f\"hr={hr[0].tolist()}\")\n",
    "            first_batch = False\n",
    "\n",
    "    return running_loss / len(dataloader.dataset),  correct / len(dataloader.dataset)\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loop = tqdm(dataloader, desc=\"Validation\", leave=True)\n",
    "        for inputs, hr, labels in loop:\n",
    "            inputs, hr, labels = inputs.to(device), hr.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs, hr)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            correct += (preds == labels).sum().item()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        return running_loss / len(dataloader.dataset), correct / len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "797e6c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    }
   ],
   "source": [
    "# START TRAINING\n",
    "model = DeepfakeDetectorHR().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0fb5f1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e8fe3e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare save path\n",
    "model_dir = 'D:/Desktop/FinalYearProject/models/rPPG_model'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "save_path = os.path.join(model_dir, 'best_model_celeb_df.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8553e37a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training :   0%|          | 1/622 [00:01<14:38,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sanity] batch loss=0.6881, inputs=torch.float32/-2.118..2.249, hr=[0.6207000017166138, 0.47679999470710754]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 622/622 [09:13<00:00,  1.12it/s]\n",
      "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 156/156 [02:11<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 0.2975 | Train Acc 0.9079 | Val Loss 0.2295 | Val Acc 0.9228\n",
      "ðŸ’¾ Best model saved to D:/Desktop/FinalYearProject/models/rPPG_model\\best_model_celeb_df.pth with Val Acc: 0.9228\n",
      "\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training :   0%|          | 1/622 [00:00<09:48,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sanity] batch loss=0.0851, inputs=torch.float32/-2.118..2.518, hr=[0.6000000238418579, 0.35760000348091125]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 622/622 [08:57<00:00,  1.16it/s]\n",
      "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 156/156 [02:09<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 0.2348 | Train Acc 0.9242 | Val Loss 0.2210 | Val Acc 0.9349\n",
      "ðŸ’¾ Best model saved to D:/Desktop/FinalYearProject/models/rPPG_model\\best_model_celeb_df.pth with Val Acc: 0.9349\n",
      "\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training :   0%|          | 1/622 [00:00<08:11,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sanity] batch loss=0.0615, inputs=torch.float32/-2.118..2.431, hr=[0.692300021648407, 0.5664499998092651]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 622/622 [08:54<00:00,  1.16it/s]\n",
      "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 156/156 [02:10<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 0.2120 | Train Acc 0.9290 | Val Loss 0.2117 | Val Acc 0.9317\n",
      "\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training :   0%|          | 1/622 [00:00<09:04,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sanity] batch loss=0.1213, inputs=torch.float32/-2.118..2.198, hr=[0.6428499817848206, 0.2980000078678131]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 622/622 [08:06<00:00,  1.28it/s]\n",
      "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 156/156 [01:47<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 0.1973 | Train Acc 0.9325 | Val Loss 0.1806 | Val Acc 0.9349\n",
      "\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training :   0%|          | 1/622 [00:00<09:09,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sanity] batch loss=0.0333, inputs=torch.float32/-2.118..2.379, hr=[0.75, 0.2980000078678131]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 622/622 [07:40<00:00,  1.35it/s]\n",
      "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 156/156 [01:46<00:00,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 0.1920 | Train Acc 0.9357 | Val Loss 0.2092 | Val Acc 0.9357\n",
      "ðŸ’¾ Best model saved to D:/Desktop/FinalYearProject/models/rPPG_model\\best_model_celeb_df.pth with Val Acc: 0.9357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "EPOCHS = 5\n",
    "best_val_acc = 0.0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
    "\n",
    "    train_loss, train_acc = train_one_epoch(model, train_dataloader, optimizer, criterion, device)\n",
    "    val_loss, val_acc     = validate(model, test_dataloader, criterion, device)\n",
    "\n",
    "    print(f\"Train Loss {train_loss:.4f} | Train Acc {train_acc:.4f} | \"\n",
    "          f\"Val Loss {val_loss:.4f} | Val Acc {val_acc:.4f}\")\n",
    "\n",
    "    # Save best model checkpoint\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_accuracy': best_val_acc\n",
    "        }, save_path)\n",
    "        print(f\"ðŸ’¾ Best model saved to {save_path} with Val Acc: {best_val_acc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
